{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "0_assay_0_fp_sklearn_allAlgorithm_10foldcv.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-Sn3fg8HZwP"
      },
      "source": [
        "assay_index = 23\n",
        "fp_index = 2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0nbNcqLzcUt"
      },
      "source": [
        "# Data download from GCP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTDOZqpSd2Y_",
        "outputId": "c8db3302-c225-4bd2-c727-0157ee320ecd"
      },
      "source": [
        "import pandas as pd\n",
        "!gsutil cp gs://chem_dsrc/P0_Toxcast/Data/assay_df/335_all_assay_df.csv ./"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://chem_dsrc/P0_Toxcast/Data/assay_df/335_all_assay_df.csv...\n",
            "/ [1 files][  4.2 MiB/  4.2 MiB]                                                \n",
            "Operation completed over 1 objects/4.2 MiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEh-LJPMeSEh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8025b04-02dc-43a4-94e5-6bdadff2663e"
      },
      "source": [
        "# rdkit 2020.03.3 버전 다운로드\n",
        "!pip install kora -q\n",
        "import kora.install.rdkit\n",
        "import numpy\n",
        "import pandas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit import DataStructs\n",
        "from rdkit.Chem import Draw\n",
        "from rdkit.Chem.Draw import SimilarityMaps\n",
        "from rdkit import Chem, DataStructs\n",
        "from rdkit.Chem import MACCSkeys\n",
        "\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "from imblearn.combine import SMOTEENN \n",
        "from sklearn import metrics\n",
        "import random\n",
        "import os \n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 61kB 3.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 5.9MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyJiOQdcnkrr"
      },
      "source": [
        "selected_assay_list = ['NVS_ADME_hCYP1A2' , 'NVS_ADME_hCYP2B6' , 'NCCT_TPO_AUR_dn','TOX21_AhR_LUC_Agonist', 'TOX21_Aromatase_Inhibition', 'TOX21_VDR_BLA_agonist_ratio', 'TOX21_VDR_BLA_antagonist_ratio', 'TOX21_AR_BLA_Agonist_ratio', 'TOX21_AR_BLA_Antagonist_ratio', 'TOX21_AR_LUC_MDAKB2_Agonist', 'TOX21_p53_BLA_p1_ratio', 'TOX21_p53_BLA_p2_ratio', 'TOX21_p53_BLA_p3_ratio', 'TOX21_p53_BLA_p4_ratio', 'TOX21_p53_BLA_p5_ratio', 'TOX21_HSE_BLA_agonist_ratio', 'TOX21_ELG1_LUC_Agonist', 'TOX21_ERa_BLA_Agonist_ratio', 'TOX21_ERa_BLA_Antagonist_ratio', 'TOX21_ESRE_BLA_ratio', 'TOX21_FXR_BLA_agonist_ratio', 'TOX21_FXR_BLA_antagonist_ratio', 'TOX21_GR_BLA_Agonist_ratio', 'TOX21_GR_BLA_Antagonist_ratio', 'TOX21_NFkB_BLA_agonist_ratio', 'TOX21_ARE_BLA_agonist_ratio', 'TOX21_TR_LUC_GH3_Agonist', 'TOX21_TR_LUC_GH3_Antagonist', 'TOX21_PPARg_BLA_Agonist_ratio', 'TOX21_PPARd_BLA_agonist_ratio', 'TOX21_PPARd_BLA_antagonist_ratio', 'TOX21_PPARg_BLA_antagonist_ratio']\n",
        "selected_assay_list.remove('TOX21_NFkB_BLA_agonist_ratio')\n",
        "selected_assay_list = [selected_assay_list[assay_index]]\n",
        "ffpp_list = ['maccs', 'morgan','rdkit','pattern','layerd']\n",
        "ffpp_list = [ffpp_list[fp_index]]\n",
        "\n",
        "algo_list = ['MLP','GBT','RF','LR','KNN','NB']\n",
        "\n",
        "dir_path = './result_sklearn_all_a_fp/'\n",
        "os.mkdir(dir_path)\n",
        "os.mkdir(dir_path + \"metric/\")\n",
        "\n",
        "for i in ffpp_list : \n",
        "  os.mkdir(dir_path + \"metric/\"+i)\n",
        "\n",
        "for i in ffpp_list : \n",
        "  for algo in algo_list :\n",
        "    os.mkdir(dir_path + \"metric/\"+i+'/'+algo)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rukg0wKKd99Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e5670d3-ac55-4417-a691-c620a288735d"
      },
      "source": [
        "a_p_list = []\n",
        "for p in ffpp_list :\n",
        "  for a in selected_assay_list : \n",
        "    for algo in algo_list :\n",
        "      a_p_list.append([p,a,algo]) \n",
        "\n",
        "assay_name = '335_all_assay'\n",
        "df = pd.read_csv(assay_name+\"_df.csv\")\n",
        "\n",
        "random.shuffle(a_p_list)\n",
        "a_p_list"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['maccs', 'NVS_ADME_hCYP2B6', 'LR'],\n",
              " ['maccs', 'NVS_ADME_hCYP2B6', 'NB'],\n",
              " ['maccs', 'NVS_ADME_hCYP2B6', 'MLP'],\n",
              " ['maccs', 'NVS_ADME_hCYP2B6', 'KNN'],\n",
              " ['maccs', 'NVS_ADME_hCYP2B6', 'RF'],\n",
              " ['maccs', 'NVS_ADME_hCYP2B6', 'GBT']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DgafzgCjDLU"
      },
      "source": [
        "def g_tf(ss_fp) : \n",
        "\n",
        "  \n",
        "  # ss_fp = a_p_list[10]\n",
        "  ffpp = ss_fp[0]#fingerfrint name\n",
        "  ss =ss_fp[1]#assay name\n",
        "  algo = ss_fp[2]\n",
        "\n",
        "  start_time = time.time()\n",
        "  print(ffpp ,algo, ss, 'start!_______________________________')\n",
        "\n",
        "  df_ss = df.loc[:,['smiles',ss]]\n",
        "  df_ss = df_ss.dropna()\n",
        "  assays_ss = df_ss.columns\n",
        "\n",
        "  #Smiles --> MACCS Keys\n",
        "  assay = ss\n",
        "  df_train  = df_ss[['smiles',ss] ]\n",
        "\n",
        "  smiles_list = []\n",
        "  err_smiles = []# fingerfrint로 변환시 에러 smiles\n",
        "  fps = []\n",
        "  enc_y = []\n",
        "  for index, row in df_train.iterrows():\n",
        "\n",
        "    if row[-1] == 'N' : \n",
        "      target = 0\n",
        "    else :\n",
        "      target = 1\n",
        "\n",
        "    try : \n",
        "      mol = Chem.MolFromSmiles(row['smiles'])\n",
        "      \n",
        "      if ffpp == 'maccs' :    \n",
        "          fp = MACCSkeys.GenMACCSKeys(mol)\n",
        "      elif ffpp == 'morgan' : \n",
        "          fp = Chem.AllChem.GetMorganFingerprintAsBitVect(mol, 2)\n",
        "      elif ffpp == 'rdkit' : \n",
        "          fp = Chem.RDKFingerprint(mol)\n",
        "      elif ffpp == 'pattern' : \n",
        "          fp = Chem.rdmolops.PatternFingerprint(mol)\n",
        "      elif ffpp == 'layerd' : \n",
        "          fp = Chem.rdmolops.LayeredFingerprint(mol)\n",
        "      else : \n",
        "          print(\"fingerfrint selection error!\")\n",
        "          \n",
        "      smiles_list.append(row['smiles'])   \n",
        "      fps.append(fp)\n",
        "      enc_y.append(target)\n",
        "    except : \n",
        "      err_smiles.append(row['smiles'])\n",
        "      pass\n",
        "\n",
        "  np_fps = []\n",
        "  for fp in fps:\n",
        "    arr = numpy.zeros((1,))\n",
        "    DataStructs.ConvertToNumpyArray(fp, arr)\n",
        "    np_fps.append(arr)\n",
        "\n",
        "  np_fps_array = numpy.array(np_fps)\n",
        "\n",
        "  # print(\"변환 에러난 smiles 개수 : \", len(err_smiles), err_smiles)\n",
        "\n",
        "  x_train_res, y_train_res = np_fps_array, np.array(enc_y)\n",
        "  train = pd.DataFrame(data=x_train_res)\n",
        "  train['label'] = y_train_res\n",
        "\n",
        "  d = pd.DataFrame(columns = ['smiles','Type'] )\n",
        "  d['smiles'] = smiles_list\n",
        "  d['Type'] = enc_y\n",
        "\n",
        "  #모든 물질에 대한 fingerprint를 string형태로 바꾼다(비교하기위함).\n",
        "  mychems = []\n",
        "  for item in np_fps:\n",
        "      #print(item)\n",
        "      item = np.array(item,dtype=int) #왜 굳이 두번? int형으로 추가하려면 이렇게 하면 되는구나....\n",
        "      s = item.tolist() #넘파이어레이를 리스트로 바꾼다. \n",
        "      t = [str(i) for i in s]\n",
        "      mychems.append(''.join(t))\n",
        "\n",
        "  i = 0\n",
        "  counter = 0\n",
        "  chemdict = {} #딗셔너리.\n",
        "  listofdups = []\n",
        "  for item in mychems:\n",
        "      try:\n",
        "          chemdict[str(item)].append(i)\n",
        "          #print('dup found')\n",
        "          for j in range(len(chemdict[str(item)])):\n",
        "              if (d['Type'][i] != d['Type'][chemdict[str(item)][j]]) :\n",
        "                  listofdups.append(i)\n",
        "                  counter+=1\n",
        "      except: \n",
        "          #print('adding new')\n",
        "          chemdict[str(item)] = []\n",
        "          chemdict[str(item)].append(i)\n",
        "      i+=1\n",
        "  # print(str(counter)+' '+str(len(list(set(listofdups)))))\n",
        "  #smote적용하기 전에서 listofdups를 빼면 된다!\n",
        "\n",
        "  dups_list = list(set(listofdups))\n",
        "\n",
        "  dups_list_0 = []\n",
        "  for index in dups_list : \n",
        "    if train.iloc[index].label == 0 :\n",
        "      dups_list_0.append(index)\n",
        "\n",
        "  train = train.drop(dups_list_0)\n",
        "\n",
        "  #분산이 0인 feature 제거\n",
        "  train = train.loc[ :, train.std() > 0]\n",
        "\n",
        "\n",
        "\n",
        "  # rskf = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=36851234) \n",
        "  rskf = StratifiedKFold(n_splits = 10, random_state = 42)\n",
        "  sme = SMOTEENN(random_state=42,sampling_strategy = \"minority\")\n",
        "\n",
        "  X = np.array( train.drop('label', axis =1) )\n",
        "  y = np.array(train.label)\n",
        "\n",
        "  count = 0 \n",
        "  rep = 0\n",
        "\n",
        "  metric_df = pd.DataFrame(columns = ['AUC','B_A','Recall','Precision','F1','MCC','Accuracy'])\n",
        "  metric_fold_df = pd.DataFrame(columns = ['AUC','B_A','Recall','Precision','F1','MCC','Accuracy'])\n",
        "\n",
        "  for train_index, test_index in rskf.split(X, y): \n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    if ss in ['NVS_ADME_hCYP1A2' , 'NVS_ADME_hCYP2B6' , 'NCCT_TPO_AUR_dn'] : \n",
        "      pass\n",
        "    else : \n",
        "      print(\"SMOTEENN fitiing....\",ffpp,algo,ss)\n",
        "      X_train, y_train = sme.fit_sample(X_train, list(y_train) )\n",
        "      print(\"....End fitiing\",ffpp, ss)\n",
        "\n",
        "    if algo == 'MLP' : \n",
        "      clf = MLPClassifier()\n",
        "    if algo == 'GBT' : \n",
        "      clf = GradientBoostingClassifier()\n",
        "    if algo == 'RF' : \n",
        "      clf = RandomForestClassifier()\n",
        "    if algo == 'LR' : \n",
        "      clf = LogisticRegression()\n",
        "    if algo == 'KNN' : \n",
        "      clf = KNeighborsClassifier()\n",
        "    if algo == 'NB' : \n",
        "      clf = GaussianNB()\n",
        "\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    result =  clf.predict_proba(X_test) # 첫 클래스의 예측확률, 두번째 클래스의 예측 확률 어떤것이 0, 1이지?\n",
        "    y_pred = clf.predict(X_test)\n",
        "    conf_mat = confusion_matrix(y_test,y_pred)\n",
        "    tn, fp, fn, tp = conf_mat.ravel()\n",
        "\n",
        "    auc =  roc_auc_score(y_test,result[:,1] )\n",
        "    recall = tp/(tp+fn) # 실제 1인 것 중 모델이 1이라 한것의 비율\n",
        "    specificity = tn / (fp+tn) # 실제 0인 것중 모델이 0이라한것의 비율\n",
        "    b_a = (recall + specificity) / 2\n",
        "    precision =  tp / (tp+fp)\n",
        "    F1 = 2* (precision * recall) / (precision + recall)\n",
        "    MCC = matthews_corrcoef(y_test,y_pred)\n",
        "    accuracy = clf.score(X_test,y_test)\n",
        "\n",
        "\n",
        "    metric_fold_df = metric_fold_df.append(pd.DataFrame([[auc,b_a,recall,precision,F1,MCC,accuracy]],columns = ['AUC','B_A','Recall','Precision','F1','MCC','Accuracy']) )\n",
        "    count +=1\n",
        "    # print(ffpp, ss, count)\n",
        "    if count % 10 == 0 :\n",
        "      average = metric_fold_df.mean()\n",
        "      std = metric_fold_df.std()\n",
        "      metric_fold_df = metric_fold_df.append(average.to_frame().T)\n",
        "      metric_fold_df = metric_fold_df.append(std.to_frame().T)\n",
        "      metric_fold_df = round(metric_fold_df ,4)\n",
        "      metric_fold_df.index = [str(rep) +'_' +  str(i) for i in range(10)]+ [str(rep)+'_Mean' , str(rep)+'_SD']\n",
        "      metric_df = metric_df.append(metric_fold_df)\n",
        "      metric_fold_df = pd.DataFrame(columns = ['AUC','B_A','Recall','Precision','F1','MCC','Accuracy'])\n",
        "      rep += 1\n",
        "      # print(ffpp, ss, rep)\n",
        "    \n",
        "\n",
        "  #전체 평균내기.\n",
        "  bool_list = [] \n",
        "  for i in metric_df.index :\n",
        "    if'Mean' in i  or 'SD' in i : \n",
        "      bool_list.append(False) \n",
        "    else : \n",
        "      bool_list.append(True)\n",
        "\n",
        "  average = metric_df[bool_list].mean().to_frame().T\n",
        "  average.index = ['Mean']\n",
        "  std = metric_df[bool_list].std().to_frame().T\n",
        "  std.index =['SD']\n",
        "\n",
        "  metric_df = metric_df.append(round(average,4) )\n",
        "  metric_df = metric_df.append(round(std,4))\n",
        "  metric_df.to_csv(dir_path+\"metric/\"+ffpp+'/'+algo+'/'+ss+\"_metric.csv\")\n",
        "  # end_time = time.time()\n",
        "  print(ffpp ,algo, ss, 'end','time spent',start_time - time.time())\n",
        "  "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEt5UtLJjBBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e487c8b1-255a-48f6-a653-57aa81eca797"
      },
      "source": [
        "import multiprocessing\n",
        "CPU_CORE = multiprocessing.cpu_count() # 멀티프로세싱 CPU 사용 수\n",
        "if __name__=='__main__':\n",
        "    pool = multiprocessing.Pool(processes=CPU_CORE)\n",
        "    pool.map(g_tf, a_p_list)\n",
        "    pool.close()\n",
        "    pool.join()  "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maccs NB NVS_ADME_hCYP2B6 start!_______________________________\n",
            "maccs MLP NVS_ADME_hCYP2B6 start!_______________________________\n",
            "maccs LR NVS_ADME_hCYP2B6 start!_______________________________\n",
            "maccs KNN NVS_ADME_hCYP2B6 start!_______________________________\n",
            "maccs NB NVS_ADME_hCYP2B6 end time spent -0.7131412029266357\n",
            "maccs RF NVS_ADME_hCYP2B6 start!_______________________________\n",
            "maccs KNN NVS_ADME_hCYP2B6 end time spent -0.805941104888916\n",
            "maccs GBT NVS_ADME_hCYP2B6 start!_______________________________\n",
            "maccs LR NVS_ADME_hCYP2B6 end time spent -1.9110393524169922\n",
            "maccs GBT NVS_ADME_hCYP2B6 end time spent -3.3654160499572754\n",
            "maccs RF NVS_ADME_hCYP2B6 end time spent -4.5386035442352295\n",
            "maccs MLP NVS_ADME_hCYP2B6 end time spent -6.365118026733398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OksDDct6Am74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99a28ded-cf10-4bf1-d8e2-82e99c3831d8"
      },
      "source": [
        "a_p_list"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['maccs', 'NVS_ADME_hCYP2B6', 'LR'],\n",
              " ['maccs', 'NVS_ADME_hCYP2B6', 'NB'],\n",
              " ['maccs', 'NVS_ADME_hCYP2B6', 'MLP'],\n",
              " ['maccs', 'NVS_ADME_hCYP2B6', 'KNN'],\n",
              " ['maccs', 'NVS_ADME_hCYP2B6', 'RF'],\n",
              " ['maccs', 'NVS_ADME_hCYP2B6', 'GBT']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T--sXw4zFsHF",
        "outputId": "441cdd98-8f52-4db6-a2c6-9eab83d2a8d9"
      },
      "source": [
        "for l in a_p_list : \n",
        "  csv_path = './result_sklearn_all_a_fp/metric/' + l[0] + '/' + l[2] + '/' + l[1] +'_metric.csv'\n",
        "  gcp_path = 'gs://chem_dsrc/P0_Toxcast/result_sklearn_all_a_fp/metric/' + l[0] + '/' + l[2] + '/' + l[1] +'_metric.csv'\n",
        "  !gsutil cp $csv_path $gcp_path"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://./result_sklearn_all_a_fp/metric/maccs/LR/NVS_ADME_hCYP2B6_metric.csv [Content-Type=text/csv]...\n",
            "/ [0 files][    0.0 B/  736.0 B]                                                \r/ [1 files][  736.0 B/  736.0 B]                                                \r\n",
            "Operation completed over 1 objects/736.0 B.                                      \n",
            "Copying file://./result_sklearn_all_a_fp/metric/maccs/NB/NVS_ADME_hCYP2B6_metric.csv [Content-Type=text/csv]...\n",
            "/ [1 files][  720.0 B/  720.0 B]                                                \n",
            "Operation completed over 1 objects/720.0 B.                                      \n",
            "Copying file://./result_sklearn_all_a_fp/metric/maccs/MLP/NVS_ADME_hCYP2B6_metric.csv [Content-Type=text/csv]...\n",
            "/ [1 files][  752.0 B/  752.0 B]                                                \n",
            "Operation completed over 1 objects/752.0 B.                                      \n",
            "Copying file://./result_sklearn_all_a_fp/metric/maccs/KNN/NVS_ADME_hCYP2B6_metric.csv [Content-Type=text/csv]...\n",
            "/ [1 files][  750.0 B/  750.0 B]                                                \n",
            "Operation completed over 1 objects/750.0 B.                                      \n",
            "Copying file://./result_sklearn_all_a_fp/metric/maccs/RF/NVS_ADME_hCYP2B6_metric.csv [Content-Type=text/csv]...\n",
            "/ [1 files][  736.0 B/  736.0 B]                                                \n",
            "Operation completed over 1 objects/736.0 B.                                      \n",
            "Copying file://./result_sklearn_all_a_fp/metric/maccs/GBT/NVS_ADME_hCYP2B6_metric.csv [Content-Type=text/csv]...\n",
            "/ [1 files][  738.0 B/  738.0 B]                                                \n",
            "Operation completed over 1 objects/738.0 B.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkPXeE69Gb8S"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}